---
title: "Generalized linear model"
author: Peter Smits
output: beamer_presentation
---
```{r constants, echo=FALSE, message=FALSE}
# set up the R environment
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
                        x <- def.chunk.hook(x, options)
                        ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

if(!('pacman' %in% rownames(installed.packages()))) {
  install.packages('pacman')
}
library(pacman)

p_load(readr, janitor, here)
p_load(tidyverse, broom, purrr, ggplot2)
p_load(foreign, arm)

if(!file.exists('.here')) {
  set_here(path = getwd())
}
```


## Regression

> ... a method that summarizes how the average values of a numerical *outcome* variable vary over subpopulations defined by linear functions of *predictors*. [...] Regression can be used to predict an outcome given a linear function of these predictors, and regression coefficients can be thought of as comparisons across predicted values or as comparisons among averages in the data.

\begin{small}
Gelman and Hill, 2007, p.31
\end{small}


# Linear regression

## Written out

\(N\) is number of observations. \(K\) is number of predictors plus one. \(y\) is a length \(N\) vector of observations. \(X\) is a \(N \times K\) matrix of predictors (and a column of 1s). \(\beta\) is a length \(K\) vector of regression coefficients (including intercept).

\(y \in \mathbb{R}\), \(\mu \in \mathbb{R}\), \(\sigma \in \mathbb{R}^{+}\), \(\beta_{k} \in \mathbb{R} \text{ for } k = 1, ..., K\).
\begin{align*}
y_{i} &\sim \mathcal{N}(\mu_{i}, \sigma) \\
\mu_{i} &= X_{i} \beta
\end{align*}

for \(i = 1, ..., N\).



## Interpreting regression parameters

- The intercept can only be interpreted assuming zero values for the other predictors. 

- If predictors are mean centered, the intercept is the average value of the response when all predictors are at their mean.

- Coeffcient \(\beta\) is the expected difference in \(y\) between two observations that differ by 1 in a single predictor.

- \(\sigma\) is standard deviation of dispersion around \(\mu\) (i.e. \(X \beta\)).



## Fitting a regression model

```{r fit_linear, size='tiny'}
kid_iq <- read.dta(here::here('ARM_Data', 'child.iq', 'kidiq.dta')) %>% 
  as_tibble() %>%
  clean_names()

# feature processing 
kid_iq <- 
  kid_iq %>%
  dplyr::select(-c(mom_age, mom_work)) %>%
  mutate_at(vars(-kid_score), ~ arm::rescale(., binary.inputs = '-0.5,0.5')) %>%
  mutate(mom_hsXmom_iq = mom_hs * mom_iq)

model_kidiq <- lm(kid_score ~ ., data = kid_iq)

tidy(model_kidiq) %>%
  knitr::kable(digits = 2)
```



## Inspecting a regression model

```{r inspect_linear, echo=FALSE, size='tiny', fig.width=5, fig.height=3}
fitted_kidiq <- fortify(model_kidiq)

ggplot(kid_iq, aes(x = mom_iq, y = kid_score)) +
  geom_point(size = 1) +
  geom_line(data = fitted_kidiq, 
            aes(x = mom_iq, y = .fitted),
            size = 1.5,
            colour = 'blue') +
facet_grid(. ~ mom_hs)
```



## Key assumptions

In order from most to least important...

1. Validity
2. Additivity and linearity
3. Independence of errors
4. Equal variance of errors
5. Normality of errors



# Logistic regression


## Written out

\(N\) is number of observations. \(K\) is number of predictors plus one. \(y\) is a length \(N\) vector of observations. \(X\) is a \(N \times K\) matrix of predictors (and a column of 1s). \(\beta\) is a length \(K\) vector of regression coefficients (including intercept).

\(y \in {0, 1}\), \(\theta \in [0, 1]\), \(\beta_{k} \in \mathbb{R} \text{ for } k = 1, ..., K\), and \(\text{logit}(p) = \log(p / 1 - p)\) and \(\text{logit}^{-1}(x) = \exp(x) / (1 + \exp(x))\).
\begin{align*}
y_{i} &\sim \text{Bernoulli}(\theta_{i}) \\
\theta_{i}&= \text{logit}^{-1}(X_{i} \beta)
\end{align*}

for \(i = 1, ..., N\).



## Logistic function \(\text{logit}(p) = \frac{p}{1 - p}\)

```{r logitistic, echo=FALSE, fig.width=3, fig.height=3}
df <- tibble(x = seq(from = -10, to = 10, by = .1)) %>%
  mutate(y = arm::invlogit(x))         # inverse logit

df %>%
  ggplot() +
  geom_line(aes(x = x, y = y)) +
  labs(x = 'log-odds scale', y = 'probability scale')
```



## Interpreting logistic regression parameters

- A regression coefficient describes the expected change in the response per unit difference in its predictor. 


- However, the logit function introduced into our model creates a nonlinearity makes clear interpretation challenging.



## The intercept of logistic regression

- As always, the intercept can only be interpreted assuming zero values for the other predictors. 

- If predictors are mean centered, the intercept is the average value of logit(response) when all predictors are at their mean.

- If zero is not interesting, or not even in the model, must be evaluated at some other point.



## Logistic regression coefficients near the mean of the data

- A difference of 1 in a predictor corresponds to expected change of \(\beta\) in the logit probability of the response.

- Can evaluate change in response at or near the mean value of predictor \(x\). 
  - Difference in \(\text{Pr}(y = 1)\) corresponding to adding 1 to \(x\) is \(\text{logit}^{-1}(\beta \bar{x}) - \text{logit}^{-1}(\beta (\bar{x} + 1))\).

- Or use derivative of logistic curve at central value.
  - Differentiating \(\text{logit}^{-1}(\alpha + \beta x)\) wrt \(x\) gives \(\beta \exp(\alpha + \beta x) / (1 + \exp(\alpha + \beta x))^{2}\).
  - Calculate \(\alpha + \beta x\) for central value.
  - Plug into derivative to give "change" in Pr\((y = 1)\) per small unit of "change" in \(x\).



## Coefficients as odds ratios

*Odds*: If two outcomes have the probabilities \((p, 1 - p)\), then the odds of \(p\) is \(p / (1 - p)\).



## The "divide by 4 rule"

The logistic curve is steepest at its center when \(X \beta = 0\) so that \(\text{logit}^{-1}(X \beta) = 0.5\).

The slope of the curve -- the derivative of the logistic function -- is maximized at this point and equals \(\beta \exp(0) / (1 + \exp(0))^{2} = \beta / 4\).

Thus, \(\beta / 4\) is the *maximum difference* in \(\text{Pr}(y = 1)\) corresponding to a unit difference in \(x\).



# Poisson regression

# Ordered categorical response

# Unordered categorical response

# Robust regression
